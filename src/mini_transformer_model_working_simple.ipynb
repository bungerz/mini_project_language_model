{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63f0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fa6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from my_tokenizer import CharDataset\n",
    "from my_gpt import SmolGPT\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e0a13",
   "metadata": {},
   "source": [
    "#### Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568fc8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.5, pytest-9.0.1, pluggy-1.6.0 -- C:\\Git\\learning_pytorch\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Git\\mini_project_language_model\\src\n",
      "plugins: anyio-4.10.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 6 items\n",
      "\n",
      "my_tests.py::test_tokenizer_roundtrip \u001b[32mPASSED\u001b[0m\u001b[32m                             [ 16%]\u001b[0m\n",
      "my_tests.py::test_single_attention_head \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 33%]\u001b[0m\n",
      "my_tests.py::test_multi_attention_head \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 50%]\u001b[0m\n",
      "my_tests.py::test_ffn \u001b[32mPASSED\u001b[0m\u001b[32m                                             [ 66%]\u001b[0m\n",
      "my_tests.py::test_transformer_block \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 83%]\u001b[0m\n",
      "my_tests.py::test_full_model \u001b[32mPASSED\u001b[0m\u001b[32m                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 2.05s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest my_tests.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff97b14",
   "metadata": {},
   "source": [
    "#### Get Data using DataSet / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be4be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load Shakespeare data\n",
    "with open('../data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build vocab from entire text ONCE\n",
    "vocab = sorted(list(set(text)))\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "n = int(0.8 * len(text))\n",
    "train_text, val_text = text[:n], text[n:]\n",
    "\n",
    "# Create datasets with shared vocab\n",
    "train_dataset = CharDataset(train_text, block_size=128, vocab=vocab)\n",
    "val_dataset = CharDataset(val_text, block_size=128, vocab=vocab)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "vocab_size = train_dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f197ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, device, eval_batches=50):\n",
    "    \"\"\"\n",
    "    Estimate loss on train and val sets\n",
    "    Args:\n",
    "        model: GPT model\n",
    "        train_loader: training DataLoader\n",
    "        val_loader: val DataLoader\n",
    "        device: cpu or cuda\n",
    "        eval_batches: nb of batches to average over\n",
    "    Returns:\n",
    "        a Dictionary with 'train' and 'val' losses\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = []\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= eval_batches:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bb71c",
   "metadata": {},
   "source": [
    "#### Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c0f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project was tested with 12 layers, 8 attention heads, and 768 embedding dimensions, on a single GPU.\n",
    "\n",
    "## big\n",
    "# n_embd=768\n",
    "# block_size=128\n",
    "# batch_size = 64\n",
    "# num_head=8\n",
    "# num_layers=12\n",
    "# dropout=0.1\n",
    "# learning_rate = 1e-4\n",
    "\n",
    "## small\n",
    "n_embd = 384\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "num_head = 8\n",
    "num_layers = 6\n",
    "dropout = 0.3\n",
    "learning_rate = 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069fcfa",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0e5f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Train: 4.3502 | Val: 4.3381\n",
      "Step   60 | Train: 2.6369 | Val: 2.6093\n",
      "Step   60 | Train: 2.6369 | Val: 2.6093\n",
      "Step  120 | Train: 2.5066 | Val: 2.4941\n",
      "Step  120 | Train: 2.5066 | Val: 2.4941\n",
      "Step  180 | Train: 2.4697 | Val: 2.4628\n",
      "Step  180 | Train: 2.4697 | Val: 2.4628\n",
      "Step  240 | Train: 2.4349 | Val: 2.4324\n",
      "Step  240 | Train: 2.4349 | Val: 2.4324\n",
      "Step  300 | Train: 2.4030 | Val: 2.4022\n",
      "Step  300 | Train: 2.4030 | Val: 2.4022\n",
      "Step  360 | Train: 2.3599 | Val: 2.3704\n",
      "Step  360 | Train: 2.3599 | Val: 2.3704\n",
      "Step  420 | Train: 2.3170 | Val: 2.3204\n",
      "Step  420 | Train: 2.3170 | Val: 2.3204\n",
      "Step  480 | Train: 2.2605 | Val: 2.2642\n",
      "Step  480 | Train: 2.2605 | Val: 2.2642\n",
      "Step  540 | Train: 2.1995 | Val: 2.2136\n",
      "Step  540 | Train: 2.1995 | Val: 2.2136\n",
      "Step  600 | Train: 2.1435 | Val: 2.1618\n",
      "Step  600 | Train: 2.1435 | Val: 2.1618\n",
      "Step  660 | Train: 2.0908 | Val: 2.1164\n",
      "Step  660 | Train: 2.0908 | Val: 2.1164\n",
      "Step  720 | Train: 2.0525 | Val: 2.0670\n",
      "Step  720 | Train: 2.0525 | Val: 2.0670\n",
      "Step  780 | Train: 2.0111 | Val: 2.0373\n",
      "Step  780 | Train: 2.0111 | Val: 2.0373\n",
      "Step  840 | Train: 1.9798 | Val: 2.0031\n",
      "Step  840 | Train: 1.9798 | Val: 2.0031\n",
      "Step  900 | Train: 1.9466 | Val: 1.9798\n",
      "Step  900 | Train: 1.9466 | Val: 1.9798\n",
      "Step  960 | Train: 1.9222 | Val: 1.9497\n",
      "Step  960 | Train: 1.9222 | Val: 1.9497\n",
      "Step 1020 | Train: 1.8948 | Val: 1.9227\n",
      "Step 1020 | Train: 1.8948 | Val: 1.9227\n",
      "Step 1080 | Train: 1.8689 | Val: 1.9105\n",
      "Step 1080 | Train: 1.8689 | Val: 1.9105\n",
      "Step 1140 | Train: 1.8338 | Val: 1.8723\n",
      "Step 1140 | Train: 1.8338 | Val: 1.8723\n",
      "Step 1200 | Train: 1.8209 | Val: 1.8595\n",
      "Step 1200 | Train: 1.8209 | Val: 1.8595\n",
      "Step 1260 | Train: 1.8041 | Val: 1.8362\n",
      "Step 1260 | Train: 1.8041 | Val: 1.8362\n",
      "Step 1320 | Train: 1.7739 | Val: 1.8199\n",
      "Step 1320 | Train: 1.7739 | Val: 1.8199\n",
      "Step 1380 | Train: 1.7594 | Val: 1.8047\n",
      "Step 1380 | Train: 1.7594 | Val: 1.8047\n",
      "Step 1440 | Train: 1.7413 | Val: 1.7868\n",
      "Step 1440 | Train: 1.7413 | Val: 1.7868\n",
      "Step 1500 | Train: 1.7252 | Val: 1.7761\n",
      "Step 1500 | Train: 1.7252 | Val: 1.7761\n",
      "Step 1560 | Train: 1.7143 | Val: 1.7595\n",
      "Step 1560 | Train: 1.7143 | Val: 1.7595\n",
      "Step 1620 | Train: 1.6911 | Val: 1.7485\n",
      "Step 1620 | Train: 1.6911 | Val: 1.7485\n",
      "Step 1680 | Train: 1.6821 | Val: 1.7354\n",
      "Step 1680 | Train: 1.6821 | Val: 1.7354\n",
      "Step 1740 | Train: 1.6651 | Val: 1.7156\n",
      "Step 1740 | Train: 1.6651 | Val: 1.7156\n",
      "Step 1800 | Train: 1.6535 | Val: 1.7078\n",
      "Step 1800 | Train: 1.6535 | Val: 1.7078\n",
      "Step 1860 | Train: 1.6403 | Val: 1.6949\n",
      "Step 1860 | Train: 1.6403 | Val: 1.6949\n",
      "Step 1920 | Train: 1.6279 | Val: 1.6837\n",
      "Step 1920 | Train: 1.6279 | Val: 1.6837\n",
      "Step 1980 | Train: 1.6159 | Val: 1.6729\n",
      "Step 1980 | Train: 1.6159 | Val: 1.6729\n",
      "Step 2040 | Train: 1.6152 | Val: 1.6666\n",
      "Step 2040 | Train: 1.6152 | Val: 1.6666\n",
      "Step 2100 | Train: 1.6045 | Val: 1.6577\n",
      "Step 2100 | Train: 1.6045 | Val: 1.6577\n",
      "Step 2160 | Train: 1.5895 | Val: 1.6515\n",
      "Step 2160 | Train: 1.5895 | Val: 1.6515\n",
      "Step 2220 | Train: 1.5738 | Val: 1.6287\n",
      "Step 2220 | Train: 1.5738 | Val: 1.6287\n",
      "Step 2280 | Train: 1.5719 | Val: 1.6266\n",
      "Step 2280 | Train: 1.5719 | Val: 1.6266\n",
      "Step 2340 | Train: 1.5621 | Val: 1.6145\n",
      "Step 2340 | Train: 1.5621 | Val: 1.6145\n",
      "Step 2400 | Train: 1.5549 | Val: 1.6076\n",
      "Step 2400 | Train: 1.5549 | Val: 1.6076\n",
      "Step 2460 | Train: 1.5542 | Val: 1.6068\n",
      "Step 2460 | Train: 1.5542 | Val: 1.6068\n",
      "Step 2520 | Train: 1.5436 | Val: 1.5978\n",
      "Step 2520 | Train: 1.5436 | Val: 1.5978\n",
      "Step 2580 | Train: 1.5315 | Val: 1.5889\n",
      "Step 2580 | Train: 1.5315 | Val: 1.5889\n",
      "Step 2640 | Train: 1.5233 | Val: 1.5806\n",
      "Step 2640 | Train: 1.5233 | Val: 1.5806\n",
      "Step 2700 | Train: 1.5220 | Val: 1.5706\n",
      "Step 2700 | Train: 1.5220 | Val: 1.5706\n",
      "Step 2760 | Train: 1.5114 | Val: 1.5678\n",
      "Step 2760 | Train: 1.5114 | Val: 1.5678\n",
      "Step 2820 | Train: 1.5068 | Val: 1.5579\n",
      "Step 2820 | Train: 1.5068 | Val: 1.5579\n",
      "Step 2880 | Train: 1.5020 | Val: 1.5505\n",
      "Step 2880 | Train: 1.5020 | Val: 1.5505\n",
      "Step 2940 | Train: 1.4923 | Val: 1.5463\n",
      "Step 2940 | Train: 1.4923 | Val: 1.5463\n",
      "Step 3000 | Train: 1.4899 | Val: 1.5395\n",
      "Step 3000 | Train: 1.4899 | Val: 1.5395\n",
      "Step 3060 | Train: 1.4887 | Val: 1.5451\n",
      "  (no improvement, 1/5)\n",
      "Step 3060 | Train: 1.4887 | Val: 1.5451\n",
      "  (no improvement, 1/5)\n",
      "Step 3120 | Train: 1.4803 | Val: 1.5317\n",
      "Step 3120 | Train: 1.4803 | Val: 1.5317\n",
      "Step 3180 | Train: 1.4716 | Val: 1.5309\n",
      "Step 3180 | Train: 1.4716 | Val: 1.5309\n",
      "Step 3240 | Train: 1.4711 | Val: 1.5187\n",
      "Step 3240 | Train: 1.4711 | Val: 1.5187\n",
      "Step 3300 | Train: 1.4666 | Val: 1.5146\n",
      "Step 3300 | Train: 1.4666 | Val: 1.5146\n",
      "Step 3360 | Train: 1.4555 | Val: 1.5135\n",
      "Step 3360 | Train: 1.4555 | Val: 1.5135\n",
      "Step 3420 | Train: 1.4533 | Val: 1.5132\n",
      "Step 3420 | Train: 1.4533 | Val: 1.5132\n",
      "Step 3480 | Train: 1.4509 | Val: 1.5093\n",
      "Step 3480 | Train: 1.4509 | Val: 1.5093\n",
      "Step 3540 | Train: 1.4414 | Val: 1.5019\n",
      "Step 3540 | Train: 1.4414 | Val: 1.5019\n",
      "Step 3600 | Train: 1.4439 | Val: 1.4963\n",
      "Step 3600 | Train: 1.4439 | Val: 1.4963\n",
      "Step 3660 | Train: 1.4366 | Val: 1.4955\n",
      "Step 3660 | Train: 1.4366 | Val: 1.4955\n",
      "Step 3720 | Train: 1.4288 | Val: 1.4892\n",
      "Step 3720 | Train: 1.4288 | Val: 1.4892\n",
      "Step 3780 | Train: 1.4233 | Val: 1.4832\n",
      "Step 3780 | Train: 1.4233 | Val: 1.4832\n",
      "Step 3840 | Train: 1.4272 | Val: 1.4805\n",
      "Step 3840 | Train: 1.4272 | Val: 1.4805\n",
      "Step 3900 | Train: 1.4176 | Val: 1.4731\n",
      "Step 3900 | Train: 1.4176 | Val: 1.4731\n",
      "Step 3960 | Train: 1.4151 | Val: 1.4771\n",
      "  (no improvement, 1/5)\n",
      "Step 3960 | Train: 1.4151 | Val: 1.4771\n",
      "  (no improvement, 1/5)\n",
      "Step 4020 | Train: 1.4122 | Val: 1.4674\n",
      "Step 4020 | Train: 1.4122 | Val: 1.4674\n",
      "Step 4080 | Train: 1.4123 | Val: 1.4754\n",
      "  (no improvement, 1/5)\n",
      "Step 4080 | Train: 1.4123 | Val: 1.4754\n",
      "  (no improvement, 1/5)\n",
      "Step 4140 | Train: 1.4033 | Val: 1.4630\n",
      "Step 4140 | Train: 1.4033 | Val: 1.4630\n",
      "Step 4200 | Train: 1.3984 | Val: 1.4613\n",
      "Step 4200 | Train: 1.3984 | Val: 1.4613\n",
      "Step 4260 | Train: 1.3949 | Val: 1.4576\n",
      "Step 4260 | Train: 1.3949 | Val: 1.4576\n",
      "Step 4320 | Train: 1.3900 | Val: 1.4519\n",
      "Step 4320 | Train: 1.3900 | Val: 1.4519\n",
      "Step 4380 | Train: 1.3922 | Val: 1.4545\n",
      "  (no improvement, 1/5)\n",
      "Step 4380 | Train: 1.3922 | Val: 1.4545\n",
      "  (no improvement, 1/5)\n",
      "Step 4440 | Train: 1.3907 | Val: 1.4458\n",
      "Step 4440 | Train: 1.3907 | Val: 1.4458\n",
      "Step 4500 | Train: 1.3877 | Val: 1.4465\n",
      "  (no improvement, 1/5)\n",
      "Step 4500 | Train: 1.3877 | Val: 1.4465\n",
      "  (no improvement, 1/5)\n",
      "Step 4560 | Train: 1.3786 | Val: 1.4446\n",
      "Step 4560 | Train: 1.3786 | Val: 1.4446\n",
      "Step 4620 | Train: 1.3765 | Val: 1.4377\n",
      "Step 4620 | Train: 1.3765 | Val: 1.4377\n",
      "Step 4680 | Train: 1.3752 | Val: 1.4364\n",
      "Step 4680 | Train: 1.3752 | Val: 1.4364\n",
      "Step 4740 | Train: 1.3671 | Val: 1.4317\n",
      "Step 4740 | Train: 1.3671 | Val: 1.4317\n",
      "Step 4800 | Train: 1.3672 | Val: 1.4355\n",
      "  (no improvement, 1/5)\n",
      "Step 4800 | Train: 1.3672 | Val: 1.4355\n",
      "  (no improvement, 1/5)\n",
      "Step 4860 | Train: 1.3618 | Val: 1.4362\n",
      "  (no improvement, 2/5)\n",
      "Step 4860 | Train: 1.3618 | Val: 1.4362\n",
      "  (no improvement, 2/5)\n",
      "Step 4920 | Train: 1.3583 | Val: 1.4324\n",
      "  (no improvement, 3/5)\n",
      "Step 4920 | Train: 1.3583 | Val: 1.4324\n",
      "  (no improvement, 3/5)\n",
      "Step 4980 | Train: 1.3582 | Val: 1.4264\n",
      "Step 4980 | Train: 1.3582 | Val: 1.4264\n",
      "Step 5040 | Train: 1.3582 | Val: 1.4261\n",
      "Step 5040 | Train: 1.3582 | Val: 1.4261\n",
      "Step 5100 | Train: 1.3543 | Val: 1.4229\n",
      "Step 5100 | Train: 1.3543 | Val: 1.4229\n",
      "Step 5160 | Train: 1.3564 | Val: 1.4115\n",
      "Step 5160 | Train: 1.3564 | Val: 1.4115\n",
      "Step 5220 | Train: 1.3483 | Val: 1.4181\n",
      "  (no improvement, 1/5)\n",
      "Step 5220 | Train: 1.3483 | Val: 1.4181\n",
      "  (no improvement, 1/5)\n",
      "Step 5280 | Train: 1.3492 | Val: 1.4127\n",
      "  (no improvement, 2/5)\n",
      "Step 5280 | Train: 1.3492 | Val: 1.4127\n",
      "  (no improvement, 2/5)\n",
      "Step 5340 | Train: 1.3474 | Val: 1.4108\n",
      "Step 5340 | Train: 1.3474 | Val: 1.4108\n",
      "Step 5400 | Train: 1.3331 | Val: 1.4160\n",
      "  (no improvement, 1/5)\n",
      "Step 5400 | Train: 1.3331 | Val: 1.4160\n",
      "  (no improvement, 1/5)\n",
      "Step 5460 | Train: 1.3333 | Val: 1.4093\n",
      "Step 5460 | Train: 1.3333 | Val: 1.4093\n",
      "Step 5520 | Train: 1.3316 | Val: 1.4078\n",
      "Step 5520 | Train: 1.3316 | Val: 1.4078\n",
      "Step 5580 | Train: 1.3355 | Val: 1.4087\n",
      "  (no improvement, 1/5)\n",
      "Step 5580 | Train: 1.3355 | Val: 1.4087\n",
      "  (no improvement, 1/5)\n",
      "Step 5640 | Train: 1.3237 | Val: 1.4065\n",
      "Step 5640 | Train: 1.3237 | Val: 1.4065\n",
      "Step 5700 | Train: 1.3309 | Val: 1.4050\n",
      "Step 5700 | Train: 1.3309 | Val: 1.4050\n",
      "Step 5760 | Train: 1.3253 | Val: 1.3962\n",
      "Step 5760 | Train: 1.3253 | Val: 1.3962\n",
      "Step 5820 | Train: 1.3239 | Val: 1.3977\n",
      "  (no improvement, 1/5)\n",
      "Step 5820 | Train: 1.3239 | Val: 1.3977\n",
      "  (no improvement, 1/5)\n",
      "Step 5880 | Train: 1.3226 | Val: 1.3971\n",
      "  (no improvement, 2/5)\n",
      "Step 5880 | Train: 1.3226 | Val: 1.3971\n",
      "  (no improvement, 2/5)\n",
      "Step 5940 | Train: 1.3248 | Val: 1.3959\n",
      "Step 5940 | Train: 1.3248 | Val: 1.3959\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SmolGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    num_head=num_head,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_steps = 6000\n",
    "eval_interval = training_steps // 100\n",
    "iter = 0\n",
    "\n",
    "# add early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # stop if val loss doesn't improve for 5 evals\n",
    "patience_counter = 0\n",
    "\n",
    "for x, y in train_loader:\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model, train_loader, val_loader, device)\n",
    "        print(f\"Step {iter:4d} | Train: {losses['train']:.4f} | Val: {losses['val']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  (no improvement, {patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at step {iter}\")\n",
    "                break\n",
    "        \n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    iter += 1\n",
    "    if iter >= training_steps:\n",
    "        break\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7fc19",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ef3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('../models/mini_transformer_model_simple_small.pth')\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcff8b9",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af15321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!\n",
      "What is the preparer, unhath the fast, be Edward,\n",
      "See and stander these by point life\n",
      "Erst by Gloucester's throne; new, and, for them see,\n",
      "Renses that that your Richard's eyes, and alone.\n",
      "\n",
      "MENENIUS:\n",
      "Ay, come, cut my sen.\n",
      "\n",
      "KING EDWARD IV:\n",
      "He shall offended with Clarence?\n",
      "\n",
      "SICINIUS:\n",
      "'Tis rue:\n",
      "\n",
      "MONTAGUE:\n",
      "A great; my lord, sir,\n",
      "Who enspect swifts their to title in son.\n",
      "\n",
      "KING RICHARD III:\n",
      "Let us queen him at a sun his charise,\n",
      "Since at tell us us let me at the sompician,\n",
      "If you will him, for so that\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dataset = CharDataset(text, block_size=128)\n",
    "max_new_tokens = 500\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokens = dataset.encode(context)\n",
    "    #not sure about this, wanting to change the tensor shape\n",
    "    idx = torch.tensor(tokens).view(1, len(tokens)).to(device)\n",
    "    y = model.generate(idx, max_new_tokens)\n",
    "    completion = dataset.decode(y[0].tolist())\n",
    "    print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
