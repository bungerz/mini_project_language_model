{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63f0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fa6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from my_tokenizer import CharDataset\n",
    "from my_gpt import SmolGPT\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e0a13",
   "metadata": {},
   "source": [
    "#### Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568fc8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.5, pytest-9.0.1, pluggy-1.6.0 -- C:\\Git\\learning_pytorch\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Git\\mini_project_language_model\\src\n",
      "plugins: anyio-4.10.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 6 items\n",
      "\n",
      "my_tests.py::test_tokenizer_roundtrip \u001b[32mPASSED\u001b[0m\u001b[32m                             [ 16%]\u001b[0m\n",
      "my_tests.py::test_single_attention_head \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 33%]\u001b[0m\n",
      "my_tests.py::test_multi_attention_head \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 50%]\u001b[0m\n",
      "my_tests.py::test_ffn \u001b[32mPASSED\u001b[0m\u001b[32m                                             [ 66%]\u001b[0m\n",
      "my_tests.py::test_transformer_block \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 83%]\u001b[0m\n",
      "my_tests.py::test_full_model \u001b[32mPASSED\u001b[0m\u001b[32m                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 2.05s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest my_tests.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff97b14",
   "metadata": {},
   "source": [
    "#### Get Data using DataSet / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be4be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load Shakespeare data\n",
    "with open('../data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build vocab from entire text ONCE\n",
    "vocab = sorted(list(set(text)))\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "n = int(0.8 * len(text))\n",
    "train_text, val_text = text[:n], text[n:]\n",
    "\n",
    "# Create datasets with shared vocab\n",
    "train_dataset = CharDataset(train_text, block_size=128, vocab=vocab)\n",
    "val_dataset = CharDataset(val_text, block_size=128, vocab=vocab)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "vocab_size = train_dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f197ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, device, eval_batches=50):\n",
    "    \"\"\"\n",
    "    Estimate loss on train and val sets\n",
    "    Args:\n",
    "        model: GPT model\n",
    "        train_loader: training DataLoader\n",
    "        val_loader: val DataLoader\n",
    "        device: cpu or cuda\n",
    "        eval_batches: nb of batches to average over\n",
    "    Returns:\n",
    "        a Dictionary with 'train' and 'val' losses\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = []\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= eval_batches:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bb71c",
   "metadata": {},
   "source": [
    "#### Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c0f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project was tested with 12 layers, 8 attention heads, and 768 embedding dimensions, on a single GPU.\n",
    "\n",
    "## big\n",
    "# n_embd=768\n",
    "# block_size=128\n",
    "# batch_size = 64\n",
    "# num_head=8\n",
    "# num_layers=12\n",
    "# dropout=0.1\n",
    "# learning_rate = 1e-4\n",
    "\n",
    "## small\n",
    "n_embd = 768\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "num_head = 8\n",
    "num_layers = 12\n",
    "dropout = 0.3\n",
    "learning_rate = 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069fcfa",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e5f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 | Train: 4.3082 | Val: 4.3158\n",
      "Step   60 | Train: 2.5195 | Val: 2.5087\n",
      "Step   60 | Train: 2.5195 | Val: 2.5087\n",
      "Step  120 | Train: 2.4474 | Val: 2.4487\n",
      "Step  120 | Train: 2.4474 | Val: 2.4487\n",
      "Step  180 | Train: 2.3827 | Val: 2.3872\n",
      "Step  180 | Train: 2.3827 | Val: 2.3872\n",
      "Step  240 | Train: 2.2876 | Val: 2.2977\n",
      "Step  240 | Train: 2.2876 | Val: 2.2977\n",
      "Step  300 | Train: 2.1558 | Val: 2.1765\n",
      "Step  300 | Train: 2.1558 | Val: 2.1765\n",
      "Step  360 | Train: 2.0728 | Val: 2.0949\n",
      "Step  360 | Train: 2.0728 | Val: 2.0949\n",
      "Step  420 | Train: 1.9919 | Val: 2.0169\n",
      "Step  420 | Train: 1.9919 | Val: 2.0169\n",
      "Step  480 | Train: 1.9250 | Val: 1.9537\n",
      "Step  480 | Train: 1.9250 | Val: 1.9537\n",
      "Step  540 | Train: 1.8519 | Val: 1.8960\n",
      "Step  540 | Train: 1.8519 | Val: 1.8960\n",
      "Step  600 | Train: 1.7950 | Val: 1.8414\n",
      "Step  600 | Train: 1.7950 | Val: 1.8414\n",
      "Step  660 | Train: 1.7421 | Val: 1.7949\n",
      "Step  660 | Train: 1.7421 | Val: 1.7949\n",
      "Step  720 | Train: 1.7019 | Val: 1.7580\n",
      "Step  720 | Train: 1.7019 | Val: 1.7580\n",
      "Step  780 | Train: 1.6632 | Val: 1.7160\n",
      "Step  780 | Train: 1.6632 | Val: 1.7160\n",
      "Step  840 | Train: 1.6266 | Val: 1.6922\n",
      "Step  840 | Train: 1.6266 | Val: 1.6922\n",
      "Step  900 | Train: 1.6002 | Val: 1.6541\n",
      "Step  900 | Train: 1.6002 | Val: 1.6541\n",
      "Step  960 | Train: 1.5729 | Val: 1.6336\n",
      "Step  960 | Train: 1.5729 | Val: 1.6336\n",
      "Step 1020 | Train: 1.5556 | Val: 1.6089\n",
      "Step 1020 | Train: 1.5556 | Val: 1.6089\n",
      "Step 1080 | Train: 1.5292 | Val: 1.5851\n",
      "Step 1080 | Train: 1.5292 | Val: 1.5851\n",
      "Step 1140 | Train: 1.5089 | Val: 1.5658\n",
      "Step 1140 | Train: 1.5089 | Val: 1.5658\n",
      "Step 1200 | Train: 1.5039 | Val: 1.5681\n",
      "  (no improvement, 1/5)\n",
      "Step 1200 | Train: 1.5039 | Val: 1.5681\n",
      "  (no improvement, 1/5)\n",
      "Step 1260 | Train: 1.4769 | Val: 1.5482\n",
      "Step 1260 | Train: 1.4769 | Val: 1.5482\n",
      "Step 1320 | Train: 1.4631 | Val: 1.5301\n",
      "Step 1320 | Train: 1.4631 | Val: 1.5301\n",
      "Step 1380 | Train: 1.4536 | Val: 1.5206\n",
      "Step 1380 | Train: 1.4536 | Val: 1.5206\n",
      "Step 1440 | Train: 1.4346 | Val: 1.5031\n",
      "Step 1440 | Train: 1.4346 | Val: 1.5031\n",
      "Step 1500 | Train: 1.4206 | Val: 1.4943\n",
      "Step 1500 | Train: 1.4206 | Val: 1.4943\n",
      "Step 1560 | Train: 1.4164 | Val: 1.4824\n",
      "Step 1560 | Train: 1.4164 | Val: 1.4824\n",
      "Step 1620 | Train: 1.4034 | Val: 1.4824\n",
      "  (no improvement, 1/5)\n",
      "Step 1620 | Train: 1.4034 | Val: 1.4824\n",
      "  (no improvement, 1/5)\n",
      "Step 1680 | Train: 1.3869 | Val: 1.4716\n",
      "Step 1680 | Train: 1.3869 | Val: 1.4716\n",
      "Step 1740 | Train: 1.3806 | Val: 1.4639\n",
      "Step 1740 | Train: 1.3806 | Val: 1.4639\n",
      "Step 1800 | Train: 1.3717 | Val: 1.4510\n",
      "Step 1800 | Train: 1.3717 | Val: 1.4510\n",
      "Step 1860 | Train: 1.3605 | Val: 1.4353\n",
      "Step 1860 | Train: 1.3605 | Val: 1.4353\n",
      "Step 1920 | Train: 1.3546 | Val: 1.4296\n",
      "Step 1920 | Train: 1.3546 | Val: 1.4296\n",
      "Step 1980 | Train: 1.3507 | Val: 1.4233\n",
      "Step 1980 | Train: 1.3507 | Val: 1.4233\n",
      "Step 2040 | Train: 1.3407 | Val: 1.4204\n",
      "Step 2040 | Train: 1.3407 | Val: 1.4204\n",
      "Step 2100 | Train: 1.3277 | Val: 1.4155\n",
      "Step 2100 | Train: 1.3277 | Val: 1.4155\n",
      "Step 2160 | Train: 1.3228 | Val: 1.4101\n",
      "Step 2160 | Train: 1.3228 | Val: 1.4101\n",
      "Step 2220 | Train: 1.3186 | Val: 1.3976\n",
      "Step 2220 | Train: 1.3186 | Val: 1.3976\n",
      "Step 2280 | Train: 1.3091 | Val: 1.3963\n",
      "Step 2280 | Train: 1.3091 | Val: 1.3963\n",
      "Step 2340 | Train: 1.2988 | Val: 1.4017\n",
      "  (no improvement, 1/5)\n",
      "Step 2340 | Train: 1.2988 | Val: 1.4017\n",
      "  (no improvement, 1/5)\n",
      "Step 2400 | Train: 1.2916 | Val: 1.3894\n",
      "Step 2400 | Train: 1.2916 | Val: 1.3894\n",
      "Step 2460 | Train: 1.2929 | Val: 1.3896\n",
      "  (no improvement, 1/5)\n",
      "Step 2460 | Train: 1.2929 | Val: 1.3896\n",
      "  (no improvement, 1/5)\n",
      "Step 2520 | Train: 1.2897 | Val: 1.3918\n",
      "  (no improvement, 2/5)\n",
      "Step 2520 | Train: 1.2897 | Val: 1.3918\n",
      "  (no improvement, 2/5)\n",
      "Step 2580 | Train: 1.2830 | Val: 1.3962\n",
      "  (no improvement, 3/5)\n",
      "Step 2580 | Train: 1.2830 | Val: 1.3962\n",
      "  (no improvement, 3/5)\n",
      "Step 2640 | Train: 1.2672 | Val: 1.3921\n",
      "  (no improvement, 4/5)\n",
      "Step 2640 | Train: 1.2672 | Val: 1.3921\n",
      "  (no improvement, 4/5)\n",
      "Step 2700 | Train: 1.2612 | Val: 1.3810\n",
      "Step 2700 | Train: 1.2612 | Val: 1.3810\n",
      "Step 2760 | Train: 1.2587 | Val: 1.3758\n",
      "Step 2760 | Train: 1.2587 | Val: 1.3758\n",
      "Step 2820 | Train: 1.2539 | Val: 1.3696\n",
      "Step 2820 | Train: 1.2539 | Val: 1.3696\n",
      "Step 2880 | Train: 1.2447 | Val: 1.3681\n",
      "Step 2880 | Train: 1.2447 | Val: 1.3681\n",
      "Step 2940 | Train: 1.2451 | Val: 1.3690\n",
      "  (no improvement, 1/5)\n",
      "Step 2940 | Train: 1.2451 | Val: 1.3690\n",
      "  (no improvement, 1/5)\n",
      "Step 3000 | Train: 1.2347 | Val: 1.3735\n",
      "  (no improvement, 2/5)\n",
      "Step 3000 | Train: 1.2347 | Val: 1.3735\n",
      "  (no improvement, 2/5)\n",
      "Step 3060 | Train: 1.2269 | Val: 1.3725\n",
      "  (no improvement, 3/5)\n",
      "Step 3060 | Train: 1.2269 | Val: 1.3725\n",
      "  (no improvement, 3/5)\n",
      "Step 3120 | Train: 1.2222 | Val: 1.3619\n",
      "Step 3120 | Train: 1.2222 | Val: 1.3619\n",
      "Step 3180 | Train: 1.2193 | Val: 1.3666\n",
      "  (no improvement, 1/5)\n",
      "Step 3180 | Train: 1.2193 | Val: 1.3666\n",
      "  (no improvement, 1/5)\n",
      "Step 3240 | Train: 1.2094 | Val: 1.3748\n",
      "  (no improvement, 2/5)\n",
      "Step 3240 | Train: 1.2094 | Val: 1.3748\n",
      "  (no improvement, 2/5)\n",
      "Step 3300 | Train: 1.2092 | Val: 1.3615\n",
      "Step 3300 | Train: 1.2092 | Val: 1.3615\n",
      "Step 3360 | Train: 1.2039 | Val: 1.3574\n",
      "Step 3360 | Train: 1.2039 | Val: 1.3574\n",
      "Step 3420 | Train: 1.1971 | Val: 1.3592\n",
      "  (no improvement, 1/5)\n",
      "Step 3420 | Train: 1.1971 | Val: 1.3592\n",
      "  (no improvement, 1/5)\n",
      "Step 3480 | Train: 1.1912 | Val: 1.3590\n",
      "  (no improvement, 2/5)\n",
      "Step 3480 | Train: 1.1912 | Val: 1.3590\n",
      "  (no improvement, 2/5)\n",
      "Step 3540 | Train: 1.1871 | Val: 1.3539\n",
      "Step 3540 | Train: 1.1871 | Val: 1.3539\n",
      "Step 3600 | Train: 1.1854 | Val: 1.3497\n",
      "Step 3600 | Train: 1.1854 | Val: 1.3497\n",
      "Step 3660 | Train: 1.1739 | Val: 1.3398\n",
      "Step 3660 | Train: 1.1739 | Val: 1.3398\n",
      "Step 3720 | Train: 1.1686 | Val: 1.3482\n",
      "  (no improvement, 1/5)\n",
      "Step 3720 | Train: 1.1686 | Val: 1.3482\n",
      "  (no improvement, 1/5)\n",
      "Step 3780 | Train: 1.1641 | Val: 1.3461\n",
      "  (no improvement, 2/5)\n",
      "Step 3780 | Train: 1.1641 | Val: 1.3461\n",
      "  (no improvement, 2/5)\n",
      "Step 3840 | Train: 1.1628 | Val: 1.3515\n",
      "  (no improvement, 3/5)\n",
      "Step 3840 | Train: 1.1628 | Val: 1.3515\n",
      "  (no improvement, 3/5)\n",
      "Step 3900 | Train: 1.1547 | Val: 1.3481\n",
      "  (no improvement, 4/5)\n",
      "Step 3900 | Train: 1.1547 | Val: 1.3481\n",
      "  (no improvement, 4/5)\n",
      "Step 3960 | Train: 1.1487 | Val: 1.3504\n",
      "  (no improvement, 5/5)\n",
      "Early stopping at step 3960\n",
      "Training complete!\n",
      "Step 3960 | Train: 1.1487 | Val: 1.3504\n",
      "  (no improvement, 5/5)\n",
      "Early stopping at step 3960\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SmolGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    num_head=num_head,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_steps = 6000\n",
    "eval_interval = training_steps // 100\n",
    "iter = 0\n",
    "\n",
    "# add early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # stop if val loss doesn't improve for 5 evals\n",
    "patience_counter = 0\n",
    "\n",
    "for x, y in train_loader:\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model, train_loader, val_loader, device)\n",
    "        print(f\"Step {iter:4d} | Train: {losses['train']:.4f} | Val: {losses['val']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  (no improvement, {patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at step {iter}\")\n",
    "                break\n",
    "        \n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    iter += 1\n",
    "    if iter >= training_steps:\n",
    "        break\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e0968",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96845b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('../models/mini_transformer_model_simple_big.pth')\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcff8b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8af15321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! O hope, for I pray to thy brother,\n",
      "I prithee my hand--\n",
      "\n",
      "PAULINA:\n",
      "I do put him doth\n",
      "My lord of a cousing duty,\n",
      "But he had 'twas, heard me pity, and mother\n",
      "I determine. Never here more, relived to sleep.\n",
      "\n",
      "ROMEO:\n",
      "The sun of my ghost: he didst he doth greets me appear\n",
      "Thy was forbid, with me came gorment to kiss;\n",
      "His courage years in a lay cast,\n",
      "To make him pace to late the day of cointrade,\n",
      "And, tell thee them in her.\n",
      "\n",
      "Provost:\n",
      "\n",
      "Son:\n",
      "Good where I all.\n",
      "\n",
      "JULIET:\n",
      "My husband, for sovereign, being dayi\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dataset = CharDataset(text, block_size=128)\n",
    "max_new_tokens = 500\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokens = dataset.encode(context)\n",
    "    #not sure about this, wanting to change the tensor shape\n",
    "    idx = torch.tensor(tokens).view(1, len(tokens)).to(device)\n",
    "    y = model.generate(idx, max_new_tokens)\n",
    "    completion = dataset.decode(y[0].tolist())\n",
    "    print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
